{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D98CddQuwKG"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import keyboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "import gym\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8rGMlN2uzgk"
   },
   "outputs": [],
   "source": [
    "ENVIRONMENT = \"PongDeterministic-v4\"\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "SAVE_MODELS = False  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 10  # Save models at every X epoch\n",
    "TRAIN_MODEL = False  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = True  # Load model from file\n",
    "LOAD_FILE_EPISODE = 900  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 100000  # Max episode\n",
    "MAX_STEP = 100000 # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 50000  # Max memory len\n",
    "MIN_MEMORY_LEN = 40000  # Min memory len before start train\n",
    "\n",
    "GAMMA = 0.97  # Discount rate\n",
    "ALPHA = 0.00025  # Learning rate\n",
    "EPSILON_DECAY = 0.99  # Epsilon decay rate by step\n",
    "\n",
    "RENDER_GAME_WINDOW = True  # Opens a new window to render the game (Won't work on colab default)\n",
    "FPS = 3\n",
    "\n",
    "LOAD_FILE_EPISODE = 900  # Load Xth episode from file\n",
    "START_VIEW = 100\n",
    "EPSILON = 0.0  # Epsilon\n",
    "RENDER_GYM_WINDOW = False  # Opens a new window to render the game (Won't work on colab default)\n",
    "RENDER_CV_WINDOW = True\n",
    "\n",
    "# Occlusion HYPERPARAMETERS\n",
    "THRESHOLD=0.0\n",
    "MODE='action' # 'value' , 'action' or 'advantage', if not 'value', parameter action=ACTION needs to be valid\n",
    "# 'value' refers to the value estimation in the network, advantage stands for the advantage estimation and 'action' stands for the final logits\n",
    "# The MODE determines what values will be used to compute the occlusion maps\n",
    "# MODE can only be 'value' or 'action' for gradient based saliency maps\n",
    "ACTION=-1 # set -1 if you want saliency map for the whole action advantage vector/whole output vector of the network if you did not set mode='value'\n",
    "CHOSENACTION=False # If this is true, ACTION will be updated each frame with the action that the agent chose last\n",
    "TYPE='PosNeg' # Currently 'Positive', 'Negative', 'PosNeg' or 'Absolute'. Only makes sense for gradient based saliency maps\n",
    "CONCURRENT = True # If true, all regions are occluded at the same time in the 4 frames. If false, seperate maps for each frame is generated.\n",
    "LAG=0 # WHICH FRAME YOU WANT TO GET SALIENCY FOR. 0 for most recent frame, -1 for average.\n",
    "METHOD=\"Gaussian-Blur\" # Currently \"Box\" or \"Gaussian-Blur\". If \"Box\" parameters Size,and Color must be set\n",
    "METRIC=\"Norm\" # What value to compute from logits\n",
    "SIZE=2.0\n",
    "COLOR=None # Grayscale value between 0 and 1 for the occlusion box color, if set to None, the average pixel value of the image will be used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxF5-bzUu1q-"
   },
   "outputs": [],
   "source": [
    "class DuelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Duel Algo. https://arxiv.org/abs/1511.06581\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, output_size):\n",
    "        super(DuelCNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.advantageEstimation = torch.empty(0, device=DEVICE, dtype=torch.float)\n",
    "        self.valueEstimation = torch.empty(0, device=DEVICE, dtype=torch.float)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
    "        linear_input_size = convw * convh * 64  # Last conv layer's out sizes\n",
    "\n",
    "        # Action layer\n",
    "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Alrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
    "\n",
    "        # State Value layer\n",
    "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
    "        self.Vlrelu = nn.LeakyReLU()  # Linear 1 activation funct\n",
    "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1)  # Only 1 node\n",
    "\n",
    "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
    "        \"\"\"\n",
    "        Calcs conv layers output image sizes\n",
    "        \"\"\"\n",
    "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
    "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
    "        return next_w, next_h\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x))\n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "        self.advantageEstimation = Ax.clone()\n",
    "\n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "        self.valueEstimation = Vx.clone()\n",
    "\n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q\n",
    "\n",
    "    # Seperates the network computation into 4 graphs:\n",
    "    # x->postConv1=self.preReLU1->self.postConv2=self.preReLU2->self.postConv3=self.preReLU3->y\n",
    "    def guidedforward(self, x):\n",
    "        self.postConv1 = self.bn1(self.conv1(x))\n",
    "        self.preReLU1 = self.postConv1.detach().clone().requires_grad_(True)\n",
    "        self.postConv2 = self.bn2(self.conv2(F.relu(self.preReLU1)))\n",
    "        self.preReLU2 = self.postConv2.detach().clone().requires_grad_(True)\n",
    "        self.postConv3 = self.bn3(self.conv3(F.relu(self.preReLU2)))\n",
    "        self.preReLU3 = self.postConv3.detach().clone().requires_grad_(True)\n",
    "        x = F.relu(self.preReLU3)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten every batch\n",
    "\n",
    "        Ax = self.Alrelu(self.Alinear1(x))\n",
    "        Ax = self.Alinear2(Ax)  # No activation on last layer\n",
    "        self.advantageEstimation = Ax.clone()\n",
    "\n",
    "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
    "        Vx = self.Vlinear2(Vx)  # No activation on last layer\n",
    "        self.valueEstimation = Vx.clone()\n",
    "\n",
    "        q = Vx + (Ax - Ax.mean())\n",
    "\n",
    "        return q\n",
    "\n",
    "    # Inputs:\n",
    "    # x: 4 channel input to the neural network\n",
    "    #  mode= 'value' or 'advantage'\n",
    "    #   If 'value':\n",
    "    #       Return saliency map(s) associated with the value estimate\n",
    "    #   If 'advantage':\n",
    "    #       Then the input action has to be set to the index of the desired action to compute saliency map for\n",
    "    # Returns 4xHxW dimensional 4 channel saliency map normalised in [-1,1] as a whole.\n",
    "\n",
    "    def getGuidedBP(self, x, mode='value', action=None):\n",
    "        if mode != 'value':\n",
    "            if mode != 'advantage':\n",
    "                raise ValueError(\"mode needs to be 'value' or 'advantage'!\")\n",
    "            else:\n",
    "                if action == None or action < 0:\n",
    "                    raise ValueError(\"If mode=='advantage', set non-negative action index to input 'action'.\")\n",
    "        self.zero_grad()\n",
    "        inputs = torch.tensor(x, requires_grad=True, device=DEVICE, dtype=torch.float)\n",
    "        self.guidedforward(inputs.unsqueeze(0))\n",
    "        if mode == 'value':\n",
    "            self.valueEstimation.backward()\n",
    "        else:\n",
    "            self.advantageEstimation[0][action].backward()\n",
    "        self.postConv3.backward(gradient=F.threshold(self.preReLU3.grad, 0.0, 0.0))\n",
    "        self.postConv2.backward(gradient=F.threshold(self.preReLU2.grad, 0.0, 0.0))\n",
    "        self.postConv1.backward(gradient=F.threshold(self.preReLU1.grad, 0.0, 0.0))\n",
    "        saliency = inputs.grad.clone()\n",
    "        AbsSaliency = torch.abs(saliency.clone())\n",
    "        saliency = saliency / torch.max(AbsSaliency)\n",
    "        return saliency\n",
    "\n",
    "    # Inputs:\n",
    "    # x: 4 channel input to the neural network\n",
    "    #  mode= 'value' or 'advantage'\n",
    "    #   If 'value':\n",
    "    #       Return saliency map(s) associated with the value estimate\n",
    "    #   If 'advantage':\n",
    "    #       Then the input action has to be set to the index of the desired action to compute saliency map for\n",
    "    # Returns 4xHxW dimensional 4 channel saliency map normalised in [-1,1] as a whole.\n",
    "\n",
    "    def getSaliencyMap(self, x, mode='value', action=None):\n",
    "        if mode != 'value':\n",
    "            if mode != 'advantage':\n",
    "                raise ValueError(\"mode needs to be 'value' or 'advantage'!\")\n",
    "            else:\n",
    "                if action == None or action < 0:\n",
    "                    raise ValueError(\"If mode=='advantage', set non-negative action index to input 'action'.\")\n",
    "        self.zero_grad()\n",
    "        inputs = torch.tensor(x, requires_grad=True, device=DEVICE, dtype=torch.float)\n",
    "        self.forward(inputs.unsqueeze(0))\n",
    "        if mode == 'value':\n",
    "            self.valueEstimation.backward()\n",
    "        else:\n",
    "            self.advantageEstimation[0][action].backward()\n",
    "        saliency = inputs.grad.clone()\n",
    "        AbsSaliency = torch.abs(saliency.clone())\n",
    "        saliency = saliency / torch.max(AbsSaliency)\n",
    "        return saliency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plT51MPbu5U5"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.original_h = 210\n",
    "        self.original_w = 160\n",
    "        self.target_h = 80  # Height after process\n",
    "        self.target_w = 64  # Widht after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0,\n",
    "                         self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model = DuelCNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
    "\n",
    "    def preProcess(self, image, singleChannel=False):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        if not singleChannel:\n",
    "            frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "            frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "            frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "\n",
    "        else:\n",
    "            frame=image\n",
    "\n",
    "        frame = frame.reshape(self.target_w, self.target_h)\n",
    "        if not singleChannel:\n",
    "            frame = frame / 255.0 # Normalize\n",
    "        # cv2.imshow(\"5\",frame)\n",
    "        # plt.show()\n",
    "        # cv2.waitKey()\n",
    "        return frame\n",
    "\n",
    "    def postProcess(self, frame):\n",
    "        img = frame.reshape(self.target_h, self.target_w)  # * 255  # Normalize\n",
    "        return img\n",
    "\n",
    "    def computeActivationDifference(self, state, occluded, mode='value', action=None, metric=\"KL\"):\n",
    "        if mode != 'value':\n",
    "            if mode != 'advantage' and mode != 'action':\n",
    "                raise ValueError(\"mode needs to be 'value', 'action' or 'advantage'!\")\n",
    "            else:\n",
    "                if action == None or action < -1:\n",
    "                    raise ValueError(\"If mode=='advantage' or 'action', set non-negative action index or -1 to input 'action'.\")\n",
    "        baseline=self.online_model.forward(torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0))\n",
    "        if mode=='value':\n",
    "            baseline=self.online_model.valueEstimation.detach().cpu()\n",
    "        elif mode=='advantage':\n",
    "            baseline=self.online_model.advantageEstimation.detach().cpu()\n",
    "\n",
    "        occl=self.online_model.forward(torch.tensor(occluded, dtype=torch.float, device=DEVICE).unsqueeze(0))\n",
    "        if mode=='value':\n",
    "            occl=self.online_model.valueEstimation.detach().cpu()\n",
    "        elif mode=='advantage':\n",
    "            occl=self.online_model.advantageEstimation.detach().cpu()\n",
    "        diff=baseline-occl\n",
    "        if (mode=='advantage' or mode=='action') and action!=-1:\n",
    "            diff=diff[action]\n",
    "        if metric==\"KL\":\n",
    "            return KLDivergence(F.softmax(baseline,dim=1).squeeze(0),F.softmax(occl,dim=1).squeeze(0))\n",
    "        if metric==\"JS\":\n",
    "            p=F.softmax(baseline,dim=1).squeeze(0)\n",
    "            q=F.softmax(occl,dim=1).squeeze(0)\n",
    "            r=(p+q)/2.0\n",
    "            return 0.5*(KLDivergence(p,r)+KLDivergence(q,r))\n",
    "        if metric==\"Norm\":\n",
    "            return torch.linalg.norm(diff)\n",
    "\n",
    "    # STRIDE IS NOT IMPLEMENTED. RIGHT NOW STRIDE=SIZE IS ASSUMED.\n",
    "    def getBoxOcclusion(self, state, mode='value', action=None, size=3, color=None, concurrent=False, metric=\"KL\"):\n",
    "        if color is None:\n",
    "            color = np.mean(state, axis=(0, 1, 2))\n",
    "        shape=self.postProcess(state[0]).shape\n",
    "        imgs=np.zeros((4, shape[0], shape[1]))\n",
    "        imgs[0]=self.postProcess(state[0])\n",
    "\n",
    "        retimg=torch.zeros(imgs.shape) # Tensor the same shape as 4 frames of grayscale inputs.  If concurrent=True, all 4 maps are identical.\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                box=np.zeros((4,shape[0],shape[1]))\n",
    "                newstates=np.zeros(state.shape)\n",
    "                if concurrent:\n",
    "                    for k in range(4):\n",
    "                        x_left = max(math.ceil(i - (size / 2)), 0)\n",
    "                        x_right = min(math.ceil(i + (size / 2)), shape[0])\n",
    "                        y_top = max(math.ceil(j - (size / 2)), 0)\n",
    "                        y_bottom = min(math.ceil(j + (size / 2)), shape[1])\n",
    "                        box[k, x_left:x_right, y_top:y_bottom] = np.ones((x_right - x_left, y_bottom - y_top)) * color\n",
    "                    states=np.copy(imgs)\n",
    "                    states[box>0]=box[box>0] # Occlusion\n",
    "                    newstates[0]=self.preProcess(states[0], singleChannel=True)\n",
    "\n",
    "                    # COMPUTE ACTIVATION DIFFERENCE\n",
    "                    sal=self.computeActivationDifference(state, newstates, mode=mode, action=action, metric=metric)\n",
    "                    # RECORD SALIENCY\n",
    "                    for k in range(4):\n",
    "                        retimg[k,i,j] = sal\n",
    "                else:\n",
    "                    for k in range(4):\n",
    "                        x_left = max(math.floor(i-(size/2)), 0)\n",
    "                        x_right = min(math.ceil(i+(size/2)), shape[0])\n",
    "                        y_top = max(math.floor(j-(size/2)), 0)\n",
    "                        y_bottom = min(math.ceil(j+(size/2)), shape[1])\n",
    "                        box[k, x_left:x_right, y_top:y_bottom] = np.ones((x_right - x_left, y_bottom - y_top)) * color\n",
    "                        states=np.copy(imgs)\n",
    "                        states[box>0]=box[box>0] # Occlusion\n",
    "                        newstates[0] = self.preProcess(states[0], singleChannel=True)\n",
    "\n",
    "                        # COMPUTE ACTIVATION DIFFERENCE\n",
    "                        sal=self.computeActivationDifference(state, newstates, mode=mode, action=action, metric=metric)\n",
    "                        # RECORD SALIENCY\n",
    "                        retimg[k,i,j] = sal\n",
    "                        box = np.zeros((4, shape[0], shape[1]))\n",
    "\n",
    "        return retimg\n",
    "\n",
    "    def getGaussianBlurOcclusion(self, state, mode='value', action=None, size=2.0, concurrent=False, metric=\"KL\"):\n",
    "        def gaussianBlurredState(state, size):\n",
    "            blurred = np.copy(state)\n",
    "            for k in range(1):\n",
    "                for i in range(state.shape[1]):\n",
    "                    for j in range(state.shape[2]):\n",
    "                        weighted_sum = 0\n",
    "                        normalizer = 0\n",
    "                        for x in range(max(math.ceil(i-2*size), 0), min(math.ceil(i+2*size), state.shape[1])):\n",
    "                            for y in range(max(math.ceil(j-2*size), 0), min(math.ceil(j+2*size), state.shape[2])):\n",
    "                                factor = round(np.exp(-1/(2*size) * ((i-x) ** 2 + (j-y) ** 2)), 4)\n",
    "                                weighted_sum += factor * state[k, x, y]\n",
    "                                normalizer += factor\n",
    "                        blurred[k, i, j] = weighted_sum / normalizer\n",
    "                        if i == 30 and j == 30:\n",
    "                            #print(blurred[k, i, j] - state[k, i, j])\n",
    "                        if blurred[k, i, j] == state[k, i, j]:\n",
    "                            #print('same blurred: {}, {}, {}'.format(k, i, j))\n",
    "            return blurred\n",
    "\n",
    "        shape = self.postProcess(state[0]).shape\n",
    "        imgs = np.zeros((4, shape[0], shape[1]))\n",
    "        imgs[0] = self.postProcess(state[0])\n",
    "\n",
    "\n",
    "\n",
    "        retimg = torch.zeros(imgs.shape)  # Tensor the same shape as 4 frames of grayscale inputs.  If concurrent=True, all 4 maps are identical.\n",
    "\n",
    "        blurred_states = gaussianBlurredState(imgs, size)\n",
    "\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                newimgs = np.copy(imgs)\n",
    "                if concurrent:\n",
    "                    for k in range(4):\n",
    "                        for x in range(max(math.ceil(i-2*size), 0), min(math.ceil(i+2*size), imgs.shape[1])):\n",
    "                            for y in range(max(math.ceil(j-2*size), 0), min(math.ceil(j+2*size), imgs.shape[2])):\n",
    "                                factor = np.exp(-1 / (2 * size) * ((i - x) ** 2 + (j - y) ** 2))\n",
    "                                newimgs[k, x, y] = factor * blurred_states[k, x, y] + (1 - factor) * imgs[k, x, y]\n",
    "                                if i == 30 and j == 30:\n",
    "                                    if newimgs[k, x, y] == imgs[k, x, y]:\n",
    "                                        print('same {}, {}, {}'.format(k, x, y))\n",
    "\n",
    "                    newstates = np.zeros(state.shape)\n",
    "                    newstates[0] = self.preProcess(newimgs[0], singleChannel=True)\n",
    "\n",
    "\n",
    "                    # COMPUTE ACTIVATION DIFFERENCE\n",
    "                    sal = self.computeActivationDifference(state, newstates, mode=mode, action=action, metric=metric)\n",
    "                    # print(i, j, sal)\n",
    "                    # RECORD SALIENCY\n",
    "                    for k in range(4):\n",
    "                        retimg[k, i, j] = sal\n",
    "\n",
    "        return retimg\n",
    "\n",
    "    def averageSaliencyMap(self, state, mode='value', action=None):\n",
    "        saliency = self.online_model.getSaliencyMap(state, mode=mode, action=action)\n",
    "        saliency = saliency.cpu()\n",
    "        saliency = torch.sum(saliency, dim=0)\n",
    "        saliency = saliency / 4\n",
    "        return saliency\n",
    "\n",
    "    def averageGuidedBP(self, state, mode='value', action=None):\n",
    "        saliency = self.online_model.getGuidedBP(state, mode=mode, action=action)\n",
    "        saliency = saliency.cpu()\n",
    "        saliency = torch.sum(saliency, dim=0)\n",
    "        saliency = saliency / 4\n",
    "        return saliency\n",
    "\n",
    "    def frameSaliencyMap(self, state, mode='value', action=None, lag=0):\n",
    "        saliency = self.online_model.getSaliencyMap(state, mode=mode, action=action)\n",
    "        saliency = saliency.cpu()\n",
    "        saliency = saliency[lag]\n",
    "        return saliency\n",
    "\n",
    "    def frameGuidedBP(self, state, mode='value', action=None, lag=0):\n",
    "        saliency = self.online_model.getGuidedBP(state, mode=mode, action=action)\n",
    "        saliency = saliency.cpu()\n",
    "        saliency = saliency[lag]\n",
    "        return saliency\n",
    "\n",
    "    def convertToPosNegSaliency(self, saliency):\n",
    "        possaliency = F.threshold(saliency, 0.0, 0.0)\n",
    "        negsaliency = F.threshold(-1 * saliency, 0.0, 0.0)\n",
    "        return possaliency, negsaliency\n",
    "\n",
    "    def convertToPositiveSaliency(self, saliency):\n",
    "        possaliency = F.threshold(saliency, 0.0, 0.0)\n",
    "        possaliency = possaliency / torch.max(possaliency)\n",
    "        return possaliency\n",
    "\n",
    "    def convertToNegativeSaliency(self, saliency):\n",
    "        negsaliency = F.threshold(-1 * saliency, 0.0, 0.0)\n",
    "        negsaliency = negsaliency / torch.max(negsaliency)\n",
    "        return negsaliency\n",
    "\n",
    "    def convertToAbsoluteSaliency(self, saliency):\n",
    "        return torch.abs(saliency)\n",
    "\n",
    "    def getAbsoluteSaliencyImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            saliency = self.convertToAbsoluteSaliency(self.averageSaliencyMap(state, mode=mode, action=action))\n",
    "        else:\n",
    "            saliency = self.convertToAbsoluteSaliency(self.frameSaliencyMap(state, mode=mode, action=action, lag=lag))\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        saliency = saliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            saliency = F.threshold(saliency, threshold, 0.0)\n",
    "        # saliency+=state[0]\n",
    "        atarisaliency = self.postProcess(saliency.numpy())\n",
    "        img = torch.cat([torch.tensor(atarisaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getAbsoluteGuidedBPImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            saliency = self.convertToAbsoluteSaliency(\n",
    "                self.averageGuidedBP(state, mode=mode, action=action))\n",
    "        else:\n",
    "            saliency = self.convertToAbsoluteSaliency(\n",
    "                self.frameGuidedBP(state, mode=mode, action=action, lag=lag))\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        saliency = saliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            saliency = F.threshold(saliency, threshold, 0.0)\n",
    "        # saliency += state[0]\n",
    "        atarisaliency = self.postProcess(saliency.numpy())\n",
    "        img = torch.cat([torch.tensor(atarisaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getPosNegSaliencyImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            possaliency, negsaliency = self.convertToPosNegSaliency(\n",
    "                self.averageSaliencyMap(state, mode=mode, action=action))\n",
    "        else:\n",
    "            possaliency, negsaliency = self.convertToPosNegSaliency(\n",
    "                self.frameSaliencyMap(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        #cv2.imshow(\"ataristate\",ataristate)\n",
    "        #cv2.waitKey()\n",
    "        possaliency = possaliency.cpu()\n",
    "        negsaliency = negsaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            possaliency = F.threshold(possaliency, threshold, 0.0)\n",
    "            negsaliency = F.threshold(negsaliency, threshold, 0.0)\n",
    "        # possaliency += state[0]  # Add state to saliency maps in order to get gray game image\n",
    "        # negsaliency += state[0]\n",
    "        ataripossaliency = self.postProcess(possaliency.numpy())\n",
    "        atarinegsaliency = self.postProcess(negsaliency.numpy())\n",
    "        img = torch.cat([torch.zeros(atarinegsaliency.shape, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.zeros(ataripossaliency.shape, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        # img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        #img = ataristate/np.max(ataristate)\n",
    "        return img\n",
    "\n",
    "    def getPosNegGuidedBPImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            possaliency, negsaliency = self.convertToPosNegSaliency(\n",
    "                self.averageGuidedBP(state, mode=mode, action=action))\n",
    "        else:\n",
    "            possaliency, negsaliency = self.convertToPosNegSaliency(\n",
    "                self.frameGuidedBP(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        possaliency = possaliency.cpu()\n",
    "        negsaliency = negsaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            possaliency = F.threshold(possaliency, threshold, 0.0)\n",
    "            negsaliency = F.threshold(negsaliency, threshold, 0.0)\n",
    "        # possaliency += state[0]  # Add state to saliency maps in order to get gray game image\n",
    "        # negsaliency += state[0]\n",
    "        ataripossaliency = self.postProcess(possaliency.numpy())\n",
    "        atarinegsaliency = self.postProcess(negsaliency.numpy())\n",
    "        img = torch.cat([torch.tensor(atarinegsaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataripossaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getPositiveSaliencyImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            possaliency = self.convertToPositiveSaliency(self.averageSaliencyMap(state, mode=mode, action=action))\n",
    "        else:\n",
    "            possaliency = self.convertToPositiveSaliency(\n",
    "                self.frameSaliencyMap(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        possaliency = possaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            possaliency = F.threshold(possaliency, threshold, 0.0)\n",
    "        # possaliency += state[0]  # Add state to saliency maps in order to get gray game image\n",
    "        ataripossaliency = self.postProcess(possaliency.numpy())\n",
    "        img = torch.cat([torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataripossaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getPositiveGuidedBPImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            possaliency = self.convertToPositiveSaliency(self.averageGuidedBP(state, mode=mode, action=action))\n",
    "        else:\n",
    "            possaliency = self.convertToPositiveSaliency(self.frameGuidedBP(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        possaliency = possaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            possaliency = F.threshold(possaliency, threshold, 0.0)\n",
    "        # possaliency += state[0]  # Add state to saliency maps in order to get gray game image\n",
    "        ataripossaliency = self.postProcess(possaliency.numpy())\n",
    "        img = torch.cat([torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataripossaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getNegativeSaliencyImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            negsaliency = self.convertToNegativeSaliency(self.averageSaliencyMap(state, mode=mode, action=action))\n",
    "        else:\n",
    "            negsaliency = self.convertToNegativeSaliency(\n",
    "                self.frameSaliencyMap(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        negsaliency = negsaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            negsaliency = F.threshold(negsaliency, threshold, 0.0)\n",
    "        # Add state to saliency maps in order to get gray game image\n",
    "        # negsaliency += state[0]\n",
    "        atarinegsaliency = self.postProcess(negsaliency.numpy())\n",
    "        img = torch.cat([torch.tensor(atarinegsaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getNegativeGuidedBPImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1):\n",
    "        if lag == -1:\n",
    "            negsaliency = self.convertToNegativeSaliency(self.averageGuidedBP(state, mode=mode, action=action))\n",
    "        else:\n",
    "            negsaliency = self.convertToNegativeSaliency(self.frameGuidedBP(state, mode=mode, action=action, lag=lag))\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "        negsaliency = negsaliency.cpu()\n",
    "        if threshold > 0.0:\n",
    "            negsaliency = F.threshold(negsaliency, threshold, 0.0)\n",
    "        # Add state to saliency maps in order to get gray game image\n",
    "        # negsaliency += state[0]\n",
    "        atarinegsaliency = self.postProcess(negsaliency.numpy())\n",
    "        img = torch.cat([torch.tensor(atarinegsaliency, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0),\n",
    "                         torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "        img = img / torch.max(img)\n",
    "        img = img.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img\n",
    "\n",
    "    def getOcclusionImage(self, state, atariimg, method=\"Box\", mode='value', action=None, threshold=0.0, size=3,\n",
    "                          color=None, concurrent=False, metric=\"KL\"):\n",
    "\n",
    "        ataristate = self.postProcess(state[0])\n",
    "\n",
    "        if method == \"Box\":\n",
    "            occmap = self.getBoxOcclusion(state, mode=mode, action=action, size=size, color=color, concurrent=concurrent, metric=metric)\n",
    "        elif method == \"Gaussian-Blur\":\n",
    "            occmap = self.getGaussianBlurOcclusion(state, mode=mode, action=action, size=size, concurrent=concurrent, metric=metric)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method!\")\n",
    "\n",
    "        occmap = occmap.cpu()\n",
    "        occmap /= torch.max(occmap[0])\n",
    "        if method == \"Gaussian-Blur\":\n",
    "            occmap = occmap ** (1/2)\n",
    "        elif method == \"Box\":\n",
    "            occmap = occmap ** (1/5)\n",
    "        if threshold > 0.0:\n",
    "            occmap = F.threshold(occmap, threshold, 0.0)\n",
    "            \n",
    "        print(occmap[0])\n",
    "\n",
    "        # Add state to saliency maps in order to get gray game image\n",
    "        # negsaliency += state[0]\n",
    "        occlusion_maps=[]\n",
    "        for i in range(4):\n",
    "            map=torch.cat([occmap[i].detach().clone().unsqueeze(0),\n",
    "                             occmap[i].detach().clone().unsqueeze(0),\n",
    "                             torch.tensor(ataristate, dtype=torch.float).unsqueeze(0)])\n",
    "            #img = img / torch.max(img)\n",
    "            map = map.transpose(0, 2).transpose(0, 1).numpy()\n",
    "        # print(np.max(img))\n",
    "        #img[0:20, :] = atariimg[0:20, :] / 255.0\n",
    "            occlusion_maps.append(cv2.cvtColor(map, cv2.COLOR_RGB2BGR))\n",
    "        return occlusion_maps\n",
    "\n",
    "    def getSaliencyMapImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1, type='PosNeg'):\n",
    "        if type != 'PosNeg' and type != 'Positive' and type != 'Negative' and type != 'Absolute':\n",
    "            raise ValueError(\"type must be 'PosNeg', 'Positive', 'Negative' or 'Absolute'\")\n",
    "        elif type == 'PosNeg':\n",
    "            img = self.getPosNegSaliencyImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Positive':\n",
    "            img = self.getPositiveSaliencyImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Negative':\n",
    "            img = self.getNegativeSaliencyImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Absolute':\n",
    "            img = self.getAbsoluteSaliencyImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        return img\n",
    "\n",
    "    def getGuidedBPImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1, type='PosNeg'):\n",
    "        if type != 'PosNeg' and type != 'Positive' and type != 'Negative' and type != 'Absolute':\n",
    "            raise ValueError(\"type must be 'PosNeg', 'Positive', 'Negative' or 'Absolute'\")\n",
    "        elif type == 'PosNeg':\n",
    "            img = self.getPosNegGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Positive':\n",
    "            img = self.getPositiveGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Negative':\n",
    "            img = self.getNegativeGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Absolute':\n",
    "            img = self.getAbsoluteGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def getGaussianBlurImage(self, state, atariimg, mode='value', action=None, threshold=0.0, lag=-1, type='PosNeg'):\n",
    "        if type != 'PosNeg' and type != 'Positive' and type != 'Negative' and type != 'Absolute':\n",
    "            raise ValueError(\"type must be 'PosNeg', 'Positive', 'Negative' or 'Absolute'\")\n",
    "        elif type == 'PosNeg':\n",
    "            img = self.getPosNegGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Positive':\n",
    "            img = self.getPositiveGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Negative':\n",
    "            img = self.getNegativeGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        elif type == 'Absolute':\n",
    "            img = self.getAbsoluteGuidedBPImage(state, atariimg, mode=mode, action=action, threshold=threshold, lag=lag)\n",
    "        return img\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            return loss, max_q\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "\n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(\n",
    "            1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    'NOOP': 'x',\n",
    "    'FIRE': 'O',\n",
    "    'LEFT': '<-',\n",
    "    'RIGHT': '->',\n",
    "    'LEFTFIRE': '<-O',\n",
    "    'RIGHTFIRE': 'O->'\n",
    "}\n",
    "\n",
    "def showActionTree(env, agent, state, episode, step, number_steps_ahead):\n",
    "    SCALE = 4\n",
    "    Q_VALUE_SENSITIVITY = 30\n",
    "    \n",
    "    actions = env.unwrapped.get_action_meanings()\n",
    "    snapshot = env.ale.cloneState()\n",
    "    current_snapshot = None\n",
    "    actionTree = nx.Graph()\n",
    "    actionTree.add_node('0', pos=(0, 0))\n",
    "    \n",
    "    action = None\n",
    "    for i in range(number_steps_ahead):\n",
    "        prev_action = action\n",
    "        action = agent.act(state)\n",
    "        with torch.no_grad():\n",
    "            _state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "            q_values = agent.online_model.forward(_state)\n",
    "            q_values_softmax = torch.nn.functional.softmax(Q_VALUE_SENSITIVITY * q_values, dim=1)\n",
    "            q_values_softmax_max = torch.argmax(q_values).item()\n",
    "            \n",
    "            current_snapshot = env.ale.cloneState()\n",
    "            for j in range(len(actions)):\n",
    "                next_state, reward, done, info = env.step(j)\n",
    "                actionTree.add_node(\n",
    "                    '{}_{}'.format(str(i+1), str(j)), pos=(3 * (i+1), (len(actions) - 1) / 2 - j),\n",
    "                    image=env.ale.getScreenRGB()\n",
    "                )\n",
    "                actionTree.add_edge(\n",
    "                    '0' if i == 0 else '{}_{}'.format(str(i), str(prev_action)), \n",
    "                    '{}_{}'.format(str(i+1), str(j)), \n",
    "                    label='{}\\n{}'.format(action_dict[actions[j]], round(q_values[0, j].item(), 2)),\n",
    "                    width=max(SCALE * 4 * q_values_softmax[0, j].item(), 1)\n",
    "                )\n",
    "                env.ale.restoreState(current_snapshot)\n",
    "                \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = agent.preProcess(next_state)  # Process image\n",
    "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "            state = next_state\n",
    "    pos = nx.get_node_attributes(actionTree, 'pos')\n",
    "    fig = plt.figure(episode * MAX_STEP + step, figsize=(SCALE * 12, SCALE * 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    nx.draw(actionTree, pos=pos, width=list(nx.get_edge_attributes(actionTree, 'width').values()), node_size=0)\n",
    "    nx.draw_networkx_edge_labels(actionTree, pos=pos, font_size=SCALE * 7, edge_labels=nx.get_edge_attributes(actionTree, 'label'))\n",
    "    \n",
    "    for i in range(number_steps_ahead):\n",
    "        for j in range(len(actions)):\n",
    "            coords = ax.transData.transform((3 * (i+1), (len(actions) - 1) / 2 - j))\n",
    "            fig.figimage(actionTree.nodes['{}_{}'.format(str(i+1), str(j))]['image'], xo=coords[0] - 500, yo=coords[1] - 300, zorder=1)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    env = env.ale.restoreState(snapshot)\n",
    "    \n",
    "\n",
    "def showActionTreeV2(env, agent, state, episode, step, number_steps_ahead, number_of_best_paths, restricted_branching=False):\n",
    "    SCALE = 4\n",
    "    Q_VALUE_SENSITIVITY = 30\n",
    "    Q_VALUE_THRESHOLD = 0.5\n",
    "    \n",
    "    actions = env.unwrapped.get_action_meanings()\n",
    "    \n",
    "    def qValuesAndScreens(env, agent, state, episode, step, number_steps_ahead):\n",
    "        if number_steps_ahead == 0:\n",
    "            return []\n",
    "        \n",
    "        action_paths = []\n",
    "        \n",
    "        snapshot = env.ale.cloneState()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "            q_values = agent.online_model.forward(_state)\n",
    "            q_values_softmax = torch.nn.functional.softmax(Q_VALUE_SENSITIVITY * q_values, dim=1)\n",
    "            q_values_softmax_max = torch.max(q_values_softmax).item()\n",
    "            print(q_values_softmax)\n",
    "            print(q_values_softmax_max)\n",
    "            \n",
    "            for i in range(len(actions)):\n",
    "                if restricted_branching and q_values_softmax[0, i].item() < Q_VALUE_THRESHOLD * q_values_softmax_max:\n",
    "                    continue\n",
    "                    \n",
    "                next_state, reward, done, info = env.step(i)\n",
    "                next_state = agent.preProcess(next_state)\n",
    "                img = env.ale.getScreenRGB()\n",
    "                sub_action_paths = []\n",
    "                if not done:\n",
    "                    sub_action_paths = qValuesAndScreens(env, agent, np.stack((next_state, state[0], state[1], state[2])), episode, step + 1, number_steps_ahead - 1)\n",
    "                if len(sub_action_paths) == 0:\n",
    "                    action_paths.append([{\n",
    "                        'action': i,\n",
    "                        'q_value': q_values[0, i].item(),\n",
    "                        'q_value_softmax': q_values_softmax[0, i].item(),\n",
    "                        'img': img\n",
    "                    }])\n",
    "                else:\n",
    "                    for sub_action_path in sub_action_paths:\n",
    "                        action_paths.append([{\n",
    "                            'action': i,\n",
    "                            'q_value': q_values[0, i].item(),\n",
    "                            'q_value_softmax': q_values_softmax[0, i].item(),\n",
    "                            'img': img\n",
    "                        }] + sub_action_path)\n",
    "                env.ale.restoreState(snapshot)\n",
    "            \n",
    "        return action_paths\n",
    "    \n",
    "    def subPathsOfLayers(action_paths, number_steps_ahead, number_of_best_paths):\n",
    "        sub_paths = list(map(lambda x: set(), range(number_steps_ahead)))\n",
    "        for i in range(number_of_best_paths):\n",
    "            for j in range(number_steps_ahead):\n",
    "                sub_paths[j].add('_'.join(map(lambda x: str(x['action']), action_paths[i][0:j+1])))\n",
    "        return sub_paths\n",
    "        \n",
    "            \n",
    "    action_paths = qValuesAndScreens(env, agent, state, episode, step, number_steps_ahead)\n",
    "    \n",
    "    def lastQValue(actionPath):\n",
    "        if len(actionPath) == 0:\n",
    "            return -float('inf')\n",
    "        return actionPath[len(actionPath) - 1]['q_value']\n",
    "    \n",
    "    action_paths.sort(key=lambda x: -lastQValue(x))\n",
    "    \n",
    "    number_of_best_paths = min(number_of_best_paths, len(action_paths))\n",
    "    \n",
    "    sub_paths_of_layers = subPathsOfLayers(action_paths, number_steps_ahead, number_of_best_paths)\n",
    "    \n",
    "    actionTree = nx.Graph()\n",
    "    actionTree.add_node('_', pos=(0, 0))\n",
    "    \n",
    "    for i in range(number_of_best_paths):\n",
    "        for j in range(len(action_paths[i])):\n",
    "            x = j + 1\n",
    "            y = action_paths[i][j]['action']\n",
    "            sub_paths_in_layer = list(sub_paths_of_layers[j])\n",
    "            sub_paths_in_layer.sort()\n",
    "            path = '_'.join(map(lambda x: str(x['action']), action_paths[i][0:j+1]))\n",
    "            actionTree.add_node(\n",
    "                path, \n",
    "                pos=(3 * x, sub_paths_in_layer.index(path) * number_of_best_paths / max(len(sub_paths_in_layer) - 1, 1) - number_of_best_paths / 2),\n",
    "                image=action_paths[i][j]['img']\n",
    "            )\n",
    "            actionTree.add_edge(\n",
    "                '_' if j == 0 else '_'.join(map(lambda x: str(x['action']), action_paths[i][0:j])), \n",
    "                '_'.join(map(lambda x: str(x['action']), action_paths[i][0:j+1])), \n",
    "                label='{}\\n{}'.format(action_dict[actions[action_paths[i][j]['action']]], round(action_paths[i][j]['q_value'], 2)),\n",
    "                width=max(SCALE * 4 * action_paths[i][j]['q_value_softmax'], 1)\n",
    "            )\n",
    "            \n",
    "    pos = nx.get_node_attributes(actionTree, 'pos')\n",
    "    fig = plt.figure(episode * MAX_STEP + step, figsize=(SCALE * 12, SCALE * 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    nx.draw(actionTree, pos=pos, width=list(nx.get_edge_attributes(actionTree, 'width').values()), node_size=0, with_labels=True)\n",
    "    nx.draw_networkx_edge_labels(actionTree, pos=pos, font_size=SCALE * 7, edge_labels=nx.get_edge_attributes(actionTree, 'label'))\n",
    "    \n",
    "    for i in range(number_of_best_paths):\n",
    "        for j in range(len(action_paths[i])):\n",
    "            x = j + 1\n",
    "            y = action_paths[i][j]['action']\n",
    "            sub_paths_in_layer = list(sub_paths_of_layers[j])\n",
    "            sub_paths_in_layer.sort()\n",
    "            path = '_'.join(map(lambda x: str(x['action']), action_paths[i][0:j+1]))\n",
    "            coords = ax.transData.transform((3 * x, sub_paths_in_layer.index(path) * number_of_best_paths / max(len(sub_paths_in_layer) - 1, 1) - number_of_best_paths / 2))\n",
    "            fig.figimage(actionTree.nodes[path]['image'], xo=coords[0] - 500, yo=coords[1] - 300, zorder=1)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)  \n",
    "    \n",
    "    \n",
    "def showActionTreeV3(env, agent, state, episode, step, number_steps_ahead, number_of_best_paths):\n",
    "    showActionTreeV2(env, agent, state, episode, step, number_steps_ahead, number_of_best_paths, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(ENVIRONMENT)  # Get env\n",
    "agent = Agent(environment)  # Create Agent\n",
    "agent.online_model.load_state_dict(torch.load(MODEL_PATH + str(LOAD_FILE_EPISODE) + \".pkl\", map_location=\"cpu\"))\n",
    "agent.online_model.eval()\n",
    "\n",
    "\n",
    "with open(MODEL_PATH + str(LOAD_FILE_EPISODE) + '.json') as outfile:\n",
    "    param = json.load(outfile)\n",
    "    agent.epsilon = param.get('epsilon')\n",
    "    startEpisode = LOAD_FILE_EPISODE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ve4vYDe3bozg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_main_code(METHOD=\"Box\"):\n",
    "    ACTION=-1\n",
    "    for episode in range(startEpisode, MAX_EPISODE):\n",
    "        \n",
    "        startTime = time.time()  # Keep time\n",
    "        \n",
    "        state = environment.reset()  # Reset env\n",
    "        atariimg = state\n",
    "        state = agent.preProcess(state)  # Process image\n",
    "        \n",
    "        total_max_q_val = 0  # Total max q vals\n",
    "        total_reward = 0  # Total reward for each episode\n",
    "        total_loss = 0  # Total loss for each episode\n",
    "\n",
    "        # Stack state . Every state contains 4 consecutive frames\n",
    "        # We stack frames like 4 channel image\n",
    "        state = np.stack((state, state, state, state))\n",
    "\n",
    "        for step in range(MAX_STEP):\n",
    "            # Select and perform an action\n",
    "            action = agent.act(state)  # Act\n",
    "            if CHOSENACTION:\n",
    "                ACTION=action\n",
    "\n",
    "            environment.render()\n",
    "            ataristate = agent.postProcess(state[0])\n",
    "            \n",
    "\n",
    "            if METHOD==\"SaliencyMap\":\n",
    "                img0 = agent.getSaliencyMapImage(state,atariimg,mode=MODE,action=ACTION,threshold=THRESHOLD,lag=0,type=TYPE)\n",
    "\n",
    "            elif METHOD==\"GuidedBP\":\n",
    "                img0 = agent.getGuidedBPImage(state,atariimg,mode=MODE,action=ACTION,threshold=THRESHOLD,lag=0,type=TYPE)\n",
    "\n",
    "            elif METHOD==\"Box\" or METHOD==\"Gaussian-Blur\":\n",
    "                if step>START_VIEW:\n",
    "                    img=agent.getOcclusionImage(state, atariimg, method=METHOD, mode=MODE, action=ACTION, threshold=THRESHOLD, size=SIZE, stride=STRIDE, color=COLOR, concurrent=CONCURRENT, metric=METRIC)\n",
    "\n",
    "                        \n",
    "            if step == 54:\n",
    "                showActionTreeV3(environment, agent, state, episode, step, 6, 10)\n",
    "                if step>START_VIEW:\n",
    "                    cv2.imshow(\"Game Tree\", cv2.resize(img[0], (400, 400)))\n",
    "\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)  # Observe\n",
    "            atariimg = next_state\n",
    "\n",
    "            next_state = agent.preProcess(next_state)  # Process image\n",
    "\n",
    "            # Stack state . Every state contains 4 time contionusly frames\n",
    "            # We stack frames like 4 channel image\n",
    "            next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "            \n",
    "            agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
    "\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state  # Update state\n",
    "\n",
    "            if done:  # Episode completed\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_main_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OpenAIPong-DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}